{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhang-xh/first-demo/blob/dev/%E2%80%9CLoRA_train_zly_v1_ipynb%E2%80%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKOaHmxB8YMX",
        "outputId": "0bb4ac99-c944-47a0-bd47-b4c01ec70799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Aug 25 06:05:44 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# 检查GPU环境\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入需要用到的Python工具包\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import time\n",
        "from subprocess import getoutput\n",
        "from IPython.utils import capture\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "u-kNF_oKA32P"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置项目需要的各种路径，比如基础模型存放路径、LoRA模型训练后的存放路径等\n",
        "root_dir = os.path.abspath(\"/content\")\n",
        "deps_dir = os.path.join(root_dir, \"deps\")\n",
        "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
        "training_dir = os.path.join(root_dir, \"LoRA\")\n",
        "pretrained_model = os.path.join(root_dir, \"pretrained_model\")\n",
        "vae_dir = os.path.join(root_dir, \"vae\")\n",
        "config_dir = os.path.join(training_dir, \"config\")\n",
        "\n",
        "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "tools_dir = os.path.join(repo_dir, \"tools\")\n",
        "finetune_dir = os.path.join(repo_dir, \"finetune\")\n",
        "\n",
        "# TODO：在新的notebook中，这段代码可以删除\n",
        "# for store in [\n",
        "#     \"root_dir\",\n",
        "#     \"deps_dir\",\n",
        "#     \"repo_dir\",\n",
        "#     \"training_dir\",\n",
        "#     \"pretrained_model\",\n",
        "#     \"vae_dir\",\n",
        "#     \"accelerate_config\",\n",
        "#     \"tools_dir\",\n",
        "#     \"finetune_dir\",\n",
        "#     \"config_dir\",\n",
        "# ]:\n",
        "#     with capture.capture_output() as cap:\n",
        "#         del cap"
      ],
      "metadata": {
        "id": "1Kd7tyYH80PG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*安装基础环境*"
      ],
      "metadata": {
        "id": "tnHUIBth9RzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_url = \"https://github.com/Linaqruf/kohya-trainer\"\n",
        "bitsandytes_main_py = \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py\"\n",
        "branch = \"\"\n",
        "install_xformers = True\n",
        "verbose = False\n",
        "\n",
        "def read_file(filename):\n",
        "    with open(filename, \"r\") as f:\n",
        "        contents = f.read()\n",
        "    return contents\n",
        "\n",
        "\n",
        "def write_file(filename, contents):\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(contents)\n",
        "\n",
        "\n",
        "def clone_repo(url):\n",
        "    if not os.path.exists(repo_dir):\n",
        "        os.chdir(root_dir)\n",
        "        !git clone {url} {repo_dir}\n",
        "    else:\n",
        "        os.chdir(repo_dir)\n",
        "        !git pull origin {branch} if branch else !git pull\n",
        "\n",
        "\n",
        "def install_dependencies():\n",
        "    s = getoutput('nvidia-smi')\n",
        "\n",
        "    !pip install {'-q' if not verbose else ''} --upgrade -r requirements.txt\n",
        "    !pip install {'-q' if not verbose else ''} torch==2.0.0+cu118 torchvision==0.15.1+cu118 torchaudio==2.0.1+cu118 torchtext==0.15.1 torchdata==0.6.0 --extra-index-url https://download.pytorch.org/whl/cu118 -U\n",
        "\n",
        "    if install_xformers:\n",
        "        !pip install {'-q' if not verbose else ''} xformers==0.0.19 triton==2.0.0 -U\n",
        "\n",
        "    from accelerate.utils import write_basic_config\n",
        "\n",
        "    if not os.path.exists(accelerate_config):\n",
        "        write_basic_config(save_location=accelerate_config)\n",
        "\n",
        "\n",
        "def main():\n",
        "    os.chdir(root_dir)\n",
        "\n",
        "    for dir in [\n",
        "        deps_dir,\n",
        "        training_dir,\n",
        "        config_dir,\n",
        "        pretrained_model,\n",
        "        vae_dir\n",
        "    ]:\n",
        "        os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "    clone_repo(repo_url)\n",
        "\n",
        "    os.chdir(repo_dir)\n",
        "    !apt --fix-broken install\n",
        "    !apt install aria2 {'-qq' if not verbose else ''}\n",
        "\n",
        "    install_dependencies()\n",
        "    time.sleep(3)\n",
        "\n",
        "    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "    os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
        "    os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "\n",
        "    cuda_path = \"/usr/local/cuda-11.8/targets/x86_64-linux/lib/\"\n",
        "    ld_library_path = os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
        "    os.environ[\"LD_LIBRARY_PATH\"] = f\"{ld_library_path}:{cuda_path}\"\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOH4vjKo9Bru",
        "outputId": "0728fbb8-a4dc-4ee9-bea5-cbb9770f1b41"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/kohya-trainer'...\n",
            "remote: Enumerating objects: 2496, done.\u001b[K\n",
            "remote: Counting objects: 100% (1162/1162), done.\u001b[K\n",
            "remote: Compressing objects: 100% (368/368), done.\u001b[K\n",
            "remote: Total 2496 (delta 896), reused 937 (delta 794), pack-reused 1334\u001b[K\n",
            "Receiving objects: 100% (2496/2496), 4.92 MiB | 18.71 MiB/s, done.\n",
            "Resolving deltas: 100% (1653/1653), done.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "0 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\n",
            "The following additional packages will be installed:\n",
            "  libaria2-0 libc-ares2\n",
            "The following NEW packages will be installed:\n",
            "  aria2 libaria2-0 libc-ares2\n",
            "0 upgraded, 3 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 1,513 kB of archives.\n",
            "After this operation, 5,441 kB of additional disk space will be used.\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "(Reading database ... 120831 files and directories currently installed.)\n",
            "Preparing to unpack .../libc-ares2_1.18.1-1ubuntu0.22.04.2_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.2) ...\n",
            "Selecting previously unselected package libaria2-0:amd64.\n",
            "Preparing to unpack .../libaria2-0_1.36.0-1_amd64.deb ...\n",
            "Unpacking libaria2-0:amd64 (1.36.0-1) ...\n",
            "Selecting previously unselected package aria2.\n",
            "Preparing to unpack .../aria2_1.36.0-1_amd64.deb ...\n",
            "Unpacking aria2 (1.36.0-1) ...\n",
            "Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.2) ...\n",
            "Setting up libaria2-0:amd64 (1.36.0-1) ...\n",
            "Setting up aria2 (1.36.0-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.5/191.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.1/503.1 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.8/825.8 kB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.8/599.8 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.3/761.3 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.0/115.0 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dadaptation (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lycoris-lora (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for library (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for elfinder-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.27.1 which is incompatible.\n",
            "yfinance 0.2.28 requires requests>=2.31, but you have requests 2.27.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m734.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.2/108.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.16) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*下载基础模型*"
      ],
      "metadata": {
        "id": "3AN0e9gf-L-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 下面提供了一些模型的下载链接，你可以到Hugging Face和CivitAI上获取更多模型。\n",
        "\n",
        "# \"Anything-v3-1\": \"https://huggingface.co/cag/anything-v3-1/resolve/main/anything-v3-1.safetensors\",\n",
        "# \"AnyLoRA\": \"https://huggingface.co/Linaqruf/stolen/resolve/main/pruned-models/AnyLoRA_noVae_fp16-pruned.safetensors\",\n",
        "# \"AnyLoRA-anime-mix\": \"https://huggingface.co/Lykon/AnyLoRA/resolve/main/AAM_Anylora_AnimeMix.safetensors\",\n",
        "# \"AnimePastelDream\": \"https://huggingface.co/Lykon/AnimePastelDream/resolve/main/AnimePastelDream_Soft_noVae_fp16.safetensors\",\n",
        "# \"Chillout-mix\": \"https://huggingface.co/Linaqruf/stolen/resolve/main/pruned-models/chillout_mix-pruned.safetensors\",\n",
        "# \"OpenJourney-v4\": \"https://huggingface.co/prompthero/openjourney-v4/resolve/main/openjourney-v4.ckpt\",\n",
        "# \"Stable-Diffusion-v1-5\": \"https://huggingface.co/Linaqruf/stolen/resolve/main/pruned-models/stable_diffusion_1_5-pruned.safetensors\""
      ],
      "metadata": {
        "id": "YR74ThL19Z1Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 下载要用的基础模型\n",
        "# 真人风格推荐用Chillout-mix 这类模型\n",
        "# 卡通风格推荐AnyLoRA、Anything这类模型\n",
        "pretrained_model_name_or_path = \"/content/pretrained_model/chillout_mix-pruned.safetensors\"\n",
        "!wget -c https://huggingface.co/Linaqruf/stolen/resolve/main/pruned-models/chillout_mix-pruned.safetensors -O $pretrained_model_name_or_path\n",
        "\n",
        "\n",
        "# pretrained_model_name_or_path = \"/content/pretrained_model/AnyLoRA_noVae_fp16-pruned.safetensors\"\n",
        "# !wget -c https://huggingface.co/Linaqruf/stolen/resolve/main/pruned-models/AnyLoRA_noVae_fp16-pruned.safetensors -O $pretrained_model_name_or_path\n",
        "\n",
        "# pretrained_model_name_or_path = \"/content/pretrained_model/moyou.safetensors\"\n",
        "# !wget -c https://huggingface.co/wind1/MoYou/resolve/main/%E9%99%90%E6%97%B6%E7%8B%AC%E5%8D%A0%EF%B8%B1%E5%A2%A8%E5%B9%BD%E4%BA%BA%E9%80%A0%E4%BA%BA_v1010%E5%AE%8C%E6%95%B4.safetensors -O $pretrained_model_name_or_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDArHtcG-ZyZ",
        "outputId": "2cd8eef4-d2b9-4251-e3d7-a267925c0c46"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-25 06:25:55--  https://huggingface.co/Linaqruf/stolen/resolve/main/pruned-models/chillout_mix-pruned.safetensors\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.4, 18.172.134.88, 18.172.134.24, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/5a/ee/5aee850f9c8f5f5951a0b13b59f0d0ae7bbd0ede690503120d02c76550f28b4c/2051a5b4f0fed7f78cf4cbeddaaa7d0dea930cba46b2af0a7182295f5c512e2d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27chillout_mix-pruned.safetensors%3B+filename%3D%22chillout_mix-pruned.safetensors%22%3B&Expires=1693195411&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MzE5NTQxMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy81YS9lZS81YWVlODUwZjljOGY1ZjU5NTFhMGIxM2I1OWYwZDBhZTdiYmQwZWRlNjkwNTAzMTIwZDAyYzc2NTUwZjI4YjRjLzIwNTFhNWI0ZjBmZWQ3Zjc4Y2Y0Y2JlZGRhYWE3ZDBkZWE5MzBjYmE0NmIyYWYwYTcxODIyOTVmNWM1MTJlMmQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=FfYKoutt2Icc-9QUOLQftVjhSHYzgwgTuQEe2Tnq5%7EpFukOkPNvQaP6gF7-HCSmZofs-kYWxFYEogL2iph0NeyM%7EJAmkh0jNox8Nmvv4AvysfdCIooEZcLf5BhkE6w1TBHlQs9R%7E4lwA9Oom-463RhOpu0DoQ-uJ99Q18n5mF0y9e3dqY0uWncjcAXcmTawcqW3JWtwtNIm2QDEpqnbKFMNXdqJ5zrKb1dVvwaXTlWLDH2i68ZykX6a9ti4OM2yR1FDtXSgts-eCOdj%7EOyrfleaSir9Y5aloWrvAeG88Mm9uTgbZWiftuRIVI5-EGl7-uqwtgEpz7F1Zjvs7jnF%7EIw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-08-25 06:25:55--  https://cdn-lfs.huggingface.co/repos/5a/ee/5aee850f9c8f5f5951a0b13b59f0d0ae7bbd0ede690503120d02c76550f28b4c/2051a5b4f0fed7f78cf4cbeddaaa7d0dea930cba46b2af0a7182295f5c512e2d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27chillout_mix-pruned.safetensors%3B+filename%3D%22chillout_mix-pruned.safetensors%22%3B&Expires=1693195411&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MzE5NTQxMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy81YS9lZS81YWVlODUwZjljOGY1ZjU5NTFhMGIxM2I1OWYwZDBhZTdiYmQwZWRlNjkwNTAzMTIwZDAyYzc2NTUwZjI4YjRjLzIwNTFhNWI0ZjBmZWQ3Zjc4Y2Y0Y2JlZGRhYWE3ZDBkZWE5MzBjYmE0NmIyYWYwYTcxODIyOTVmNWM1MTJlMmQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=FfYKoutt2Icc-9QUOLQftVjhSHYzgwgTuQEe2Tnq5%7EpFukOkPNvQaP6gF7-HCSmZofs-kYWxFYEogL2iph0NeyM%7EJAmkh0jNox8Nmvv4AvysfdCIooEZcLf5BhkE6w1TBHlQs9R%7E4lwA9Oom-463RhOpu0DoQ-uJ99Q18n5mF0y9e3dqY0uWncjcAXcmTawcqW3JWtwtNIm2QDEpqnbKFMNXdqJ5zrKb1dVvwaXTlWLDH2i68ZykX6a9ti4OM2yR1FDtXSgts-eCOdj%7EOyrfleaSir9Y5aloWrvAeG88Mm9uTgbZWiftuRIVI5-EGl7-uqwtgEpz7F1Zjvs7jnF%7EIw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.154.185.26, 18.154.185.27, 18.154.185.64, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.154.185.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4265096689 (4.0G) [binary/octet-stream]\n",
            "Saving to: ‘/content/pretrained_model/chillout_mix-pruned.safetensors’\n",
            "\n",
            "/content/pretrained 100%[===================>]   3.97G   108MB/s    in 26s     \n",
            "\n",
            "2023-08-25 06:26:21 (157 MB/s) - ‘/content/pretrained_model/chillout_mix-pruned.safetensors’ saved [4265096689/4265096689]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "准备数据"
      ],
      "metadata": {
        "id": "dwbHq_3k-ri1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 将准备的照片（10图以上）放在这个目录下\n",
        "train_data_dir = os.path.join(root_dir, \"LoRA/train_data/zly_cartoon\")\n",
        "os.makedirs(train_data_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Your train data directory : {train_data_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fa4sdqtY-i6G",
        "outputId": "06d20fc6-4e6c-49f0-b3cb-e0980cbd944c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your train data directory : /content/LoRA/train_data/zly_cartoon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用BLIP模型给你的图片加上prompt，用于训练。\n",
        "# BLIP是一个多模态生成算法，输入图片，得到图片的prompt描述信息。\n",
        "\n",
        "import os\n",
        "\n",
        "os.chdir(finetune_dir)\n",
        "\n",
        "batch_size = 6\n",
        "max_data_loader_n_workers = 2\n",
        "beam_search = True\n",
        "min_length = 5\n",
        "max_length = 75\n",
        "recursive = False\n",
        "verbose_logging = True\n",
        "\n",
        "config = {\n",
        "    \"_train_data_dir\" : train_data_dir,\n",
        "    \"batch_size\" : batch_size,\n",
        "    \"beam_search\" : beam_search,\n",
        "    \"min_length\" : min_length,\n",
        "    \"max_length\" : max_length,\n",
        "    \"debug\" : verbose_logging,\n",
        "    \"caption_extension\" : \".caption\",\n",
        "    \"max_data_loader_n_workers\" : max_data_loader_n_workers,\n",
        "    \"recursive\" : recursive\n",
        "}\n",
        "\n",
        "args = \"\"\n",
        "for k, v in config.items():\n",
        "    if k.startswith(\"_\"):\n",
        "        args += f'\"{v}\" '\n",
        "    elif isinstance(v, str):\n",
        "        args += f'--{k}=\"{v}\" '\n",
        "    elif isinstance(v, bool) and v:\n",
        "        args += f\"--{k} \"\n",
        "    elif isinstance(v, float) and not isinstance(v, bool):\n",
        "        args += f\"--{k}={v} \"\n",
        "    elif isinstance(v, int) and not isinstance(v, bool):\n",
        "        args += f\"--{k}={v} \"\n",
        "\n",
        "final_args = f\"python make_captions.py {args}\"\n",
        "\n",
        "os.chdir(finetune_dir)\n",
        "!{final_args}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bKEjOIe-0Sj",
        "outputId": "fb995d05-7afc-4cd3-8f5b-76c504bdbb8d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.16) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
            "load images from /content/LoRA/train_data/zly_cartoon\n",
            "found 24 images.\n",
            "loading BLIP caption: https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\n",
            "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 7.31MB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 221kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 570/570 [00:00<00:00, 3.04MB/s]\n",
            "100% 1.66G/1.66G [00:12<00:00, 141MB/s]\n",
            "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\n",
            "BLIP loaded\n",
            "  0% 0/4 [00:00<?, ?it/s]/content/LoRA/train_data/zly_cartoon/03a92b7bc8b27d13f070a964d9d7b094.png a woman with long hair wearing a blue dress\n",
            "/content/LoRA/train_data/zly_cartoon/0413ad5acedb3001793abf17b6f45856.jpg a woman in a blue and white dress holding a fan\n",
            "/content/LoRA/train_data/zly_cartoon/05239fc7a628a7bc71e929d704fd4e03.jpeg a woman in a white dress riding a horse\n",
            "/content/LoRA/train_data/zly_cartoon/0ea76c122983c91f1c23a68994a46921.jpeg a woman in a red dress holding a red umbrella\n",
            "/content/LoRA/train_data/zly_cartoon/20c1127621e2dc4de0a9366283533d37.jpeg a woman in a white dress with a sword\n",
            "/content/LoRA/train_data/zly_cartoon/319c1cf81091206870666af2db54a5fc.jpeg a woman in a blue and white dress sitting down\n",
            " 25% 1/4 [00:09<00:28,  9.55s/it]/content/LoRA/train_data/zly_cartoon/32ec57fc43e41d7d9d0b09170cb4d78f.jpeg a woman with a blue headpiece and earrings\n",
            "/content/LoRA/train_data/zly_cartoon/35be3fc2f0a5dfae0955b58184110852.jpeg a woman in a white dress with a veil\n",
            "/content/LoRA/train_data/zly_cartoon/365a59cc48f76d2da56b38707e57f8f8.jpeg a woman in a white dress holding a green bottle\n",
            "/content/LoRA/train_data/zly_cartoon/650682a974252674b80e5654ec8ee10c.jpeg a woman with a red necklace and a white shirt\n",
            "/content/LoRA/train_data/zly_cartoon/685a2d6782133dd92abc98cc4f07e7b7.jpg a woman in a green kimono is posing for a picture\n",
            "/content/LoRA/train_data/zly_cartoon/89fc8fd2b110e8ccaccd8741792d38c6.jpeg a woman in a kimono is holding a red umbrella\n",
            " 50% 2/4 [00:10<00:10,  5.45s/it]/content/LoRA/train_data/zly_cartoon/8e6983e2b361c33a77cd7a3882a5ec6d.jpeg a woman with a black jacket and red lipstick\n",
            "/content/LoRA/train_data/zly_cartoon/9af6c0978a1f272f04260d654f6660f9.jpeg a woman in a white dress posing for a picture\n",
            "/content/LoRA/train_data/zly_cartoon/ab1da3109c4d4471daddcb128a9ae8bb.jpeg a woman in a blue and white dress\n",
            "/content/LoRA/train_data/zly_cartoon/ab97192c6489f6d0695867a725f105f6.jpg a woman with a blue dress and a red necklace\n",
            "/content/LoRA/train_data/zly_cartoon/af6cd8d98ee4a2864b0082f6a2066faa.jpeg a woman in a black dress with a watch\n",
            "/content/LoRA/train_data/zly_cartoon/bc050c6a76e586880157a1c4ec9121f2.jpeg a woman in a red dress holding a book\n",
            " 75% 3/4 [00:12<00:04,  4.05s/it]/content/LoRA/train_data/zly_cartoon/d4e70f02d5007411db3ad9d3c4d5ee88.jpeg a woman in a yellow robe and a white hood\n",
            "/content/LoRA/train_data/zly_cartoon/de668df8f73adc83130ab8b16e43560f.jpeg a woman in a blue dress holding her hand up\n",
            "/content/LoRA/train_data/zly_cartoon/e488748fe39b7c8071946bd04f35d03d.jpeg a woman with a red jacket and a red lipstick\n",
            "/content/LoRA/train_data/zly_cartoon/e5697e008a0eec23e678e858e3f0a9f1.jpeg a woman in a white dress sitting on a grass\n",
            "/content/LoRA/train_data/zly_cartoon/e5eefdd1507fc38e12a1bf1f1703630b.jpeg a woman in a blue dress holding a bird\n",
            "/content/LoRA/train_data/zly_cartoon/f6f7962d5ad68155752ee6c7e6b96980.jpeg a woman in a white dress holding a glass\n",
            "100% 4/4 [00:13<00:00,  3.37s/it]\n",
            "done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "训练配置"
      ],
      "metadata": {
        "id": "H-oNLmEyMic3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "project_name = \"zly_pro\"\n",
        "vae = \"\" # 使用基础模型中的VAE\n",
        "output_dir = os.path.join(root_dir, \"LoRA/output/zly_pro\")\n",
        "\n",
        "sample_dir = os.path.join(output_dir, \"sample\")\n",
        "for dir in [output_dir, sample_dir]:\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "print(\"Project Name: \", project_name)\n",
        "print(\n",
        "    \"Pretrained Model Path: \", pretrained_model_name_or_path\n",
        ") if pretrained_model_name_or_path else print(\"No Pretrained Model path specified.\")\n",
        "\n",
        "print(\"VAE Path: \", vae) if vae else print(\"No VAE path specified.\")\n",
        "print(\"Output Path: \", output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMv2gmZZMg-C",
        "outputId": "806d081c-39f7-4fda-8f67-67cfc27ead9d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project Name:  zly_pro\n",
            "Pretrained Model Path:  /content/pretrained_model/chillout_mix-pruned.safetensors\n",
            "No VAE path specified.\n",
            "Output Path:  /content/LoRA/output/zly_pro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 这段代码用于预处理数据，将我们的训练数据、正则化数据处理成训练模型能用的dataloader\n",
        "# 数据增广是非常关键的操作，比如图像翻转、图像颜色扰动、图像朝着目标风格的迁移等。这些都是后期可调的。\n",
        "\n",
        "import os\n",
        "import toml\n",
        "import glob\n",
        "\n",
        "dataset_repeats = 20\n",
        "activation_word = \"\"\n",
        "caption_extension = \".caption\"\n",
        "resolution = 512\n",
        "flip_aug = True\n",
        "keep_tokens = 0\n",
        "\n",
        "def parse_folder_name(folder_name, default_num_repeats, default_class_token):\n",
        "    folder_name_parts = folder_name.split(\"_\")\n",
        "\n",
        "    if len(folder_name_parts) == 2:\n",
        "        if folder_name_parts[0].isdigit():\n",
        "            num_repeats = int(folder_name_parts[0])\n",
        "            class_token = folder_name_parts[1].replace(\"_\", \" \")\n",
        "        else:\n",
        "            num_repeats = default_num_repeats\n",
        "            class_token = default_class_token\n",
        "    else:\n",
        "        num_repeats = default_num_repeats\n",
        "        class_token = default_class_token\n",
        "\n",
        "    return num_repeats, class_token\n",
        "\n",
        "def find_image_files(path):\n",
        "    supported_extensions = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")\n",
        "    return [file for file in glob.glob(path + '/**/*', recursive=True) if file.lower().endswith(supported_extensions)]\n",
        "\n",
        "def process_data_dir(data_dir, default_num_repeats, default_class_token, is_reg=False):\n",
        "    subsets = []\n",
        "\n",
        "    images = find_image_files(data_dir)\n",
        "    if images:\n",
        "        subsets.append({\n",
        "            \"image_dir\": data_dir,\n",
        "            \"class_tokens\": default_class_token,\n",
        "            \"num_repeats\": default_num_repeats,\n",
        "            **({\"is_reg\": is_reg} if is_reg else {}),\n",
        "        })\n",
        "\n",
        "    for root, dirs, files in os.walk(data_dir):\n",
        "        for folder in dirs:\n",
        "            folder_path = os.path.join(root, folder)\n",
        "            images = find_image_files(folder_path)\n",
        "\n",
        "            if images:\n",
        "                num_repeats, class_token = parse_folder_name(folder, default_num_repeats, default_class_token)\n",
        "\n",
        "                subset = {\n",
        "                    \"image_dir\": folder_path,\n",
        "                    \"class_tokens\": class_token,\n",
        "                    \"num_repeats\": num_repeats,\n",
        "                }\n",
        "\n",
        "                if is_reg:\n",
        "                    subset[\"is_reg\"] = True\n",
        "\n",
        "                subsets.append(subset)\n",
        "\n",
        "    return subsets\n",
        "\n",
        "\n",
        "train_subsets = process_data_dir(train_data_dir, dataset_repeats, activation_word)\n",
        "print(train_subsets)\n",
        "# reg_subsets = process_data_dir(reg_data_dir, dataset_repeats, activation_word, is_reg=True)\n",
        "\n",
        "# subsets = train_subsets + reg_subsets\n",
        "subsets = train_subsets\n",
        "\n",
        "config = {\n",
        "    \"general\": {\n",
        "        \"enable_bucket\": True,\n",
        "        \"caption_extension\": caption_extension,\n",
        "        \"shuffle_caption\": True,\n",
        "        \"keep_tokens\": keep_tokens,\n",
        "        \"bucket_reso_steps\": 64,\n",
        "        \"bucket_no_upscale\": False,\n",
        "    },\n",
        "    \"datasets\": [\n",
        "        {\n",
        "            \"resolution\": resolution,\n",
        "            \"min_bucket_reso\": 320 if resolution > 640 else 256,\n",
        "            \"max_bucket_reso\": 1280 if resolution > 640 else 1024,\n",
        "            \"caption_dropout_rate\": 0,\n",
        "            \"caption_tag_dropout_rate\": 0,\n",
        "            \"caption_dropout_every_n_epochs\": 0,\n",
        "            \"flip_aug\": flip_aug,\n",
        "            \"color_aug\": False,\n",
        "            \"face_crop_aug_range\": None,\n",
        "            \"subsets\": subsets,\n",
        "        }\n",
        "    ],\n",
        "}\n",
        "\n",
        "dataset_config = os.path.join(config_dir, \"dataset_config.toml\")\n",
        "\n",
        "for key in config:\n",
        "    if isinstance(config[key], dict):\n",
        "        for sub_key in config[key]:\n",
        "            if config[key][sub_key] == \"\":\n",
        "                config[key][sub_key] = None\n",
        "    elif config[key] == \"\":\n",
        "        config[key] = None\n",
        "\n",
        "config_str = toml.dumps(config)\n",
        "\n",
        "with open(dataset_config, \"w\") as f:\n",
        "    f.write(config_str)\n",
        "\n",
        "print(config_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90LnMgMD_QOO",
        "outputId": "3aedbfd3-261a-477f-f615-8a8f99eda6d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'image_dir': '/content/LoRA/train_data/zly_cartoon', 'class_tokens': '', 'num_repeats': 20}]\n",
            "[[datasets]]\n",
            "resolution = 512\n",
            "min_bucket_reso = 256\n",
            "max_bucket_reso = 1024\n",
            "caption_dropout_rate = 0\n",
            "caption_tag_dropout_rate = 0\n",
            "caption_dropout_every_n_epochs = 0\n",
            "flip_aug = true\n",
            "color_aug = false\n",
            "[[datasets.subsets]]\n",
            "image_dir = \"/content/LoRA/train_data/zly_cartoon\"\n",
            "class_tokens = \"\"\n",
            "num_repeats = 20\n",
            "\n",
            "\n",
            "[general]\n",
            "enable_bucket = true\n",
            "caption_extension = \".caption\"\n",
            "shuffle_caption = true\n",
            "keep_tokens = 0\n",
            "bucket_reso_steps = 64\n",
            "bucket_no_upscale = false\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 可以提供预训练的LoRA模型\n",
        "# 分别设置text_encoder和UNet的学习率\n",
        "\n",
        "network_category = \"LoRA\"\n",
        "\n",
        "conv_dim = 32\n",
        "conv_alpha = 16\n",
        "network_dim = 32\n",
        "network_alpha = 16\n",
        "network_weight = \"\"\n",
        "network_module = \"networks.lora\"\n",
        "network_args = \"\"\n",
        "\n",
        "min_snr_gamma = -1\n",
        "optimizer_type = \"AdamW8bit\"\n",
        "optimizer_args = \"\"\n",
        "train_unet = True\n",
        "unet_lr = 1e-4\n",
        "train_text_encoder = True\n",
        "text_encoder_lr = 5e-5\n",
        "lr_scheduler = \"constant\"\n",
        "lr_warmup_steps = 0\n",
        "lr_scheduler_num_cycles = 0\n",
        "lr_scheduler_power = 0\n",
        "\n",
        "print(\"- LoRA Config:\")\n",
        "print(f\"  - Min-SNR Weighting: {min_snr_gamma}\") if not min_snr_gamma == -1 else \"\"\n",
        "print(f\"  - Loading network module: {network_module}\")\n",
        "print(f\"  - {network_module} linear_dim set to: {network_dim}\")\n",
        "print(f\"  - {network_module} linear_alpha set to: {network_alpha}\")\n",
        "\n",
        "if not network_weight:\n",
        "    print(\"  - No LoRA weight loaded.\")\n",
        "else:\n",
        "    if os.path.exists(network_weight):\n",
        "        print(f\"  - Loading LoRA weight: {network_weight}\")\n",
        "    else:\n",
        "        print(f\"  - {network_weight} does not exist.\")\n",
        "        network_weight = \"\"\n",
        "\n",
        "print(\"- Optimizer Config:\")\n",
        "print(f\"  - Additional network category: {network_category}\")\n",
        "print(f\"  - Using {optimizer_type} as Optimizer\")\n",
        "if optimizer_args:\n",
        "    print(f\"  - Optimizer Args: {optimizer_args}\")\n",
        "if train_unet and train_text_encoder:\n",
        "    print(\"  - Train UNet and Text Encoder\")\n",
        "    print(f\"    - UNet learning rate: {unet_lr}\")\n",
        "    print(f\"    - Text encoder learning rate: {text_encoder_lr}\")\n",
        "if train_unet and not train_text_encoder:\n",
        "    print(\"  - Train UNet only\")\n",
        "    print(f\"    - UNet learning rate: {unet_lr}\")\n",
        "if train_text_encoder and not train_unet:\n",
        "    print(\"  - Train Text Encoder only\")\n",
        "    print(f\"    - Text encoder learning rate: {text_encoder_lr}\")\n",
        "print(f\"  - Learning rate warmup steps: {lr_warmup_steps}\")\n",
        "print(f\"  - Learning rate Scheduler: {lr_scheduler}\")\n",
        "if lr_scheduler == \"cosine_with_restarts\":\n",
        "    print(f\"  - lr_scheduler_num_cycles: {lr_scheduler_num_cycles}\")\n",
        "elif lr_scheduler == \"polynomial\":\n",
        "    print(f\"  - lr_scheduler_power: {lr_scheduler_power}\")"
      ],
      "metadata": {
        "id": "NSf_JvU6_0Lk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d3e179c-a1a6-4d5e-86cd-3d70339b8d04"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- LoRA Config:\n",
            "  - Loading network module: networks.lora\n",
            "  - networks.lora linear_dim set to: 32\n",
            "  - networks.lora linear_alpha set to: 16\n",
            "  - No LoRA weight loaded.\n",
            "- Optimizer Config:\n",
            "  - Additional network category: LoRA\n",
            "  - Using AdamW8bit as Optimizer\n",
            "  - Train UNet and Text Encoder\n",
            "    - UNet learning rate: 0.0001\n",
            "    - Text encoder learning rate: 5e-05\n",
            "  - Learning rate warmup steps: 0\n",
            "  - Learning rate Scheduler: constant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置存储模型的格式，我们为了适配WebUI，使用safetensors格式\n",
        "# 设置测试用prompt\n",
        "# 设置训练参数\n",
        "\n",
        "\n",
        "import toml\n",
        "import os\n",
        "\n",
        "lowram = True\n",
        "enable_sample_prompt = True\n",
        "sampler = \"ddim\"  #[\"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"]\n",
        "noise_offset = 0.0\n",
        "num_epochs = 10\n",
        "vae_batch_size = 4\n",
        "train_batch_size = 6\n",
        "mixed_precision = \"fp16\"  # [\"no\",\"fp16\",\"bf16\"]\n",
        "save_precision = \"fp16\"  #  [\"float\", \"fp16\", \"bf16\"]\n",
        "save_n_epochs_type = \"save_every_n_epochs\"\n",
        "save_n_epochs_type_value = 1\n",
        "save_model_as = \"safetensors\"  # [\"ckpt\", \"pt\", \"safetensors\"]\n",
        "max_token_length = 225\n",
        "clip_skip = 2\n",
        "gradient_checkpointing = False\n",
        "gradient_accumulation_steps = 1\n",
        "seed = -1\n",
        "logging_dir = os.path.join(root_dir, \"LoRA/logs\")\n",
        "prior_loss_weight = 1.0\n",
        "os.chdir(repo_dir)\n",
        "\n",
        "sample_str = f\"\"\"\n",
        "  masterpiece, best quality, 1girl, long hair, big eyes, looking at viewer, simple background \\\n",
        "  --n EasyNegativeV2,(badhandv4:1.2), \\\n",
        "  --w 512 \\\n",
        "  --h 512 \\\n",
        "  --l 7 \\\n",
        "  --s 28\n",
        "\"\"\"\n",
        "\n",
        "config = {\n",
        "    \"model_arguments\": {\n",
        "        \"v2\": False,\n",
        "        \"v_parameterization\": False,\n",
        "        \"pretrained_model_name_or_path\": pretrained_model_name_or_path,\n",
        "        \"vae\": vae,\n",
        "    },\n",
        "    \"additional_network_arguments\": {\n",
        "        \"no_metadata\": False,\n",
        "        \"unet_lr\": float(unet_lr) if train_unet else None,\n",
        "        \"text_encoder_lr\": float(text_encoder_lr) if train_text_encoder else None,\n",
        "        \"network_weights\": network_weight,\n",
        "        \"network_module\": network_module,\n",
        "        \"network_dim\": network_dim,\n",
        "        \"network_alpha\": network_alpha,\n",
        "        \"network_args\": network_args,\n",
        "        \"network_train_unet_only\": True if train_unet and not train_text_encoder else False,\n",
        "        \"network_train_text_encoder_only\": True if train_text_encoder and not train_unet else False,\n",
        "        \"training_comment\": None,\n",
        "    },\n",
        "    \"optimizer_arguments\": {\n",
        "        \"min_snr_gamma\": min_snr_gamma if not min_snr_gamma == -1 else None,\n",
        "        \"optimizer_type\": optimizer_type,\n",
        "        \"learning_rate\": unet_lr,\n",
        "        \"max_grad_norm\": 1.0,\n",
        "        \"optimizer_args\": eval(optimizer_args) if optimizer_args else None,\n",
        "        \"lr_scheduler\": lr_scheduler,\n",
        "        \"lr_warmup_steps\": lr_warmup_steps,\n",
        "        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
        "    },\n",
        "    \"dataset_arguments\": {\n",
        "        \"cache_latents\": True,\n",
        "        \"debug_dataset\": False,\n",
        "        \"vae_batch_size\": vae_batch_size,\n",
        "    },\n",
        "    \"training_arguments\": {\n",
        "        \"output_dir\": output_dir,\n",
        "        \"output_name\": project_name,\n",
        "        \"save_precision\": save_precision,\n",
        "        \"save_every_n_epochs\": save_n_epochs_type_value if save_n_epochs_type == \"save_every_n_epochs\" else None,\n",
        "        \"save_n_epoch_ratio\": save_n_epochs_type_value if save_n_epochs_type == \"save_n_epoch_ratio\" else None,\n",
        "        \"save_last_n_epochs\": None,\n",
        "        \"save_state\": None,\n",
        "        \"save_last_n_epochs_state\": None,\n",
        "        \"resume\": None,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        \"max_token_length\": 225,\n",
        "        \"mem_eff_attn\": False,\n",
        "        \"xformers\": True,\n",
        "        \"max_train_epochs\": num_epochs,\n",
        "        \"max_data_loader_n_workers\": 8,\n",
        "        \"persistent_data_loader_workers\": True,\n",
        "        \"seed\": seed if seed > 0 else None,\n",
        "        \"gradient_checkpointing\": gradient_checkpointing,\n",
        "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "        \"mixed_precision\": mixed_precision,\n",
        "        \"clip_skip\": clip_skip,\n",
        "        \"logging_dir\": logging_dir,\n",
        "        \"log_prefix\": project_name,\n",
        "        \"noise_offset\": noise_offset if noise_offset > 0 else None,\n",
        "        \"lowram\": lowram,\n",
        "    },\n",
        "    \"sample_prompt_arguments\": {\n",
        "        \"sample_every_n_steps\": None,\n",
        "        \"sample_every_n_epochs\": 1 if enable_sample_prompt else 999999,\n",
        "        \"sample_sampler\": sampler,\n",
        "    },\n",
        "    \"dreambooth_arguments\": {\n",
        "        \"prior_loss_weight\": 1.0,\n",
        "    },\n",
        "    \"saving_arguments\": {\n",
        "        \"save_model_as\": save_model_as\n",
        "    },\n",
        "}\n",
        "\n",
        "config_path = os.path.join(config_dir, \"config_file.toml\")\n",
        "prompt_path = os.path.join(config_dir, \"sample_prompt.txt\")\n",
        "\n",
        "\n",
        "for key in config:\n",
        "    if isinstance(config[key], dict):\n",
        "        for sub_key in config[key]:\n",
        "            if config[key][sub_key] == \"\":\n",
        "                config[key][sub_key] = None\n",
        "    elif config[key] == \"\":\n",
        "        config[key] = None\n",
        "\n",
        "config_str = toml.dumps(config)\n",
        "\n",
        "write_file(config_path, config_str)\n",
        "write_file(prompt_path, sample_str)\n",
        "\n",
        "print(config_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCrsnXwvNHQp",
        "outputId": "06e1ec96-cfa0-475d-9d9c-e2a7b39fd842"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[model_arguments]\n",
            "v2 = false\n",
            "v_parameterization = false\n",
            "pretrained_model_name_or_path = \"/content/pretrained_model/chillout_mix-pruned.safetensors\"\n",
            "\n",
            "[additional_network_arguments]\n",
            "no_metadata = false\n",
            "unet_lr = 0.0001\n",
            "text_encoder_lr = 5e-5\n",
            "network_module = \"networks.lora\"\n",
            "network_dim = 32\n",
            "network_alpha = 16\n",
            "network_train_unet_only = false\n",
            "network_train_text_encoder_only = false\n",
            "\n",
            "[optimizer_arguments]\n",
            "optimizer_type = \"AdamW8bit\"\n",
            "learning_rate = 0.0001\n",
            "max_grad_norm = 1.0\n",
            "lr_scheduler = \"constant\"\n",
            "lr_warmup_steps = 0\n",
            "\n",
            "[dataset_arguments]\n",
            "cache_latents = true\n",
            "debug_dataset = false\n",
            "vae_batch_size = 4\n",
            "\n",
            "[training_arguments]\n",
            "output_dir = \"/content/LoRA/output/zly_pro\"\n",
            "output_name = \"zly_pro\"\n",
            "save_precision = \"fp16\"\n",
            "save_every_n_epochs = 1\n",
            "train_batch_size = 6\n",
            "max_token_length = 225\n",
            "mem_eff_attn = false\n",
            "xformers = true\n",
            "max_train_epochs = 10\n",
            "max_data_loader_n_workers = 8\n",
            "persistent_data_loader_workers = true\n",
            "gradient_checkpointing = false\n",
            "gradient_accumulation_steps = 1\n",
            "mixed_precision = \"fp16\"\n",
            "clip_skip = 2\n",
            "logging_dir = \"/content/LoRA/logs\"\n",
            "log_prefix = \"zly_pro\"\n",
            "lowram = true\n",
            "\n",
            "[sample_prompt_arguments]\n",
            "sample_every_n_epochs = 1\n",
            "sample_sampler = \"ddim\"\n",
            "\n",
            "[dreambooth_arguments]\n",
            "prior_loss_weight = 1.0\n",
            "\n",
            "[saving_arguments]\n",
            "save_model_as = \"safetensors\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "开始训练！过程中可以在LoRA/output/zly_pro/sample目录下看训练过程中的图像生成效果。用于选择自己满意的LoRA模型。"
      ],
      "metadata": {
        "id": "yHMu-aazNbE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_prompt = os.path.join(config_dir, \"sample_prompt.txt\")\n",
        "config_file = os.path.join(config_dir, \"config_file.toml\")\n",
        "dataset_config = os.path.join(config_dir, \"dataset_config.toml\")\n",
        "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "\n",
        "accelerate_conf = {\n",
        "    \"config_file\" : accelerate_config,\n",
        "    \"num_cpu_threads_per_process\" : 1,\n",
        "}\n",
        "\n",
        "train_conf = {\n",
        "    \"sample_prompts\" : sample_prompt,\n",
        "    \"dataset_config\" : dataset_config,\n",
        "    \"config_file\" : config_file\n",
        "}\n",
        "\n",
        "def train(config):\n",
        "    args = \"\"\n",
        "    for k, v in config.items():\n",
        "        if k.startswith(\"_\"):\n",
        "            args += f'\"{v}\" '\n",
        "        elif isinstance(v, str):\n",
        "            args += f'--{k}=\"{v}\" '\n",
        "        elif isinstance(v, bool) and v:\n",
        "            args += f\"--{k} \"\n",
        "        elif isinstance(v, float) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "        elif isinstance(v, int) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "\n",
        "    return args\n",
        "\n",
        "accelerate_args = train(accelerate_conf)\n",
        "train_args = train(train_conf)\n",
        "final_args = f\"accelerate launch {accelerate_args} train_network.py {train_args}\"\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "!{final_args}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cz2_7bONXq_",
        "outputId": "91416da1-5884-4e9d-c55b-b8625d6ad961"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.16) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
            "/usr/local/lib/python3.10/dist-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.16) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
            "Loading settings from /content/LoRA/config/config_file.toml...\n",
            "/content/LoRA/config/config_file\n",
            "prepare tokenizer\n",
            "Downloading (…)olve/main/vocab.json: 100% 961k/961k [00:00<00:00, 17.9MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 525k/525k [00:00<00:00, 6.68MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 389/389 [00:00<00:00, 1.60MB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 905/905 [00:00<00:00, 6.22MB/s]\n",
            "update token length: 225\n",
            "Load dataset config from /content/LoRA/config/dataset_config.toml\n",
            "prepare images.\n",
            "found directory /content/LoRA/train_data/zly_cartoon contains 24 image files\n",
            "480 train images with repeating.\n",
            "0 reg images.\n",
            "no regularization images / 正則化画像が見つかりませんでした\n",
            "[Dataset 0]\n",
            "  batch_size: 6\n",
            "  resolution: (512, 512)\n",
            "  enable_bucket: True\n",
            "  min_bucket_reso: 256\n",
            "  max_bucket_reso: 1024\n",
            "  bucket_reso_steps: 64\n",
            "  bucket_no_upscale: False\n",
            "\n",
            "  [Subset 0 of Dataset 0]\n",
            "    image_dir: \"/content/LoRA/train_data/zly_cartoon\"\n",
            "    image_count: 24\n",
            "    num_repeats: 20\n",
            "    shuffle_caption: True\n",
            "    keep_tokens: 0\n",
            "    caption_dropout_rate: 0\n",
            "    caption_dropout_every_n_epoches: 0\n",
            "    caption_tag_dropout_rate: 0\n",
            "    color_aug: False\n",
            "    flip_aug: True\n",
            "    face_crop_aug_range: None\n",
            "    random_crop: False\n",
            "    token_warmup_min: 1,\n",
            "    token_warmup_step: 0,\n",
            "    is_reg: False\n",
            "    class_tokens: \n",
            "    caption_extension: .caption\n",
            "\n",
            "\n",
            "[Dataset 0]\n",
            "loading image sizes.\n",
            "100% 24/24 [00:00<00:00, 1720.09it/s]\n",
            "make buckets\n",
            "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
            "bucket 0: resolution (384, 640), count: 200\n",
            "bucket 1: resolution (448, 576), count: 240\n",
            "bucket 2: resolution (512, 512), count: 20\n",
            "bucket 3: resolution (640, 384), count: 20\n",
            "mean ar error (without repeats): 0.031675061931176814\n",
            "prepare accelerator\n",
            "Using accelerator 0.15.0 or above.\n",
            "loading model for process 0/1\n",
            "load StableDiffusion checkpoint\n",
            "loading u-net: <All keys matched successfully>\n",
            "loading vae: <All keys matched successfully>\n",
            "Downloading (…)lve/main/config.json: 100% 4.52k/4.52k [00:00<00:00, 20.2MB/s]\n",
            "Downloading pytorch_model.bin: 100% 1.71G/1.71G [00:15<00:00, 110MB/s]\n",
            "loading text encoder: <All keys matched successfully>\n",
            "Replace CrossAttention.forward to use xformers\n",
            "[Dataset 0]\n",
            "caching latents.\n",
            "100% 9/9 [00:11<00:00,  1.30s/it]\n",
            "import network module: networks.lora\n",
            "create LoRA network. base dim (rank): 32, alpha: 16\n",
            "create LoRA for Text Encoder: 72 modules.\n",
            "create LoRA for U-Net: 192 modules.\n",
            "enable LoRA for text encoder\n",
            "enable LoRA for U-Net\n",
            "prepare optimizer, data loader etc.\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
            "================================================================================\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "use 8-bit AdamW optimizer | {}\n",
            "override steps. steps for 10 epochs is / 指定エポックまでのステップ数: 820\n",
            "running training / 学習開始\n",
            "  num train images * repeats / 学習画像の数×繰り返し回数: 480\n",
            "  num reg images / 正則化画像の数: 0\n",
            "  num batches per epoch / 1epochのバッチ数: 82\n",
            "  num epochs / epoch数: 10\n",
            "  batch size per device / バッチサイズ: 6\n",
            "  gradient accumulation steps / 勾配を合計するステップ数 = 1\n",
            "  total optimization steps / 学習ステップ数: 820\n",
            "steps:   0% 0/820 [00:00<?, ?it/s]epoch 1/10\n",
            "steps:  10% 82/820 [01:55<17:15,  1.40s/it, loss=0.168]saving checkpoint: /content/LoRA/output/zly_pro/zly_pro-000001.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 82\n",
            "prompt: masterpiece, best quality, 1girl, long hair, big eyes, looking at viewer, simple background  \n",
            "negative_prompt: EasyNegativeV2,(badhandv4:1.2),  \n",
            "height: 512\n",
            "width: 512\n",
            "sample_steps: 28\n",
            "scale: 7.0\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:06,  4.29it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:05,  4.86it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:05,  4.99it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:04,  5.02it/s]\u001b[A\n",
            " 18% 5/28 [00:01<00:04,  5.03it/s]\u001b[A\n",
            " 21% 6/28 [00:01<00:04,  5.03it/s]\u001b[A\n",
            " 25% 7/28 [00:01<00:04,  5.07it/s]\u001b[A\n",
            " 29% 8/28 [00:01<00:03,  5.08it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:03,  5.10it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:03,  5.10it/s]\u001b[A\n",
            " 39% 11/28 [00:02<00:03,  5.10it/s]\u001b[A\n",
            " 43% 12/28 [00:02<00:03,  5.10it/s]\u001b[A\n",
            " 46% 13/28 [00:02<00:02,  5.11it/s]\u001b[A\n",
            " 50% 14/28 [00:02<00:02,  5.12it/s]\u001b[A\n",
            " 54% 15/28 [00:02<00:02,  5.11it/s]\u001b[A\n",
            " 57% 16/28 [00:03<00:02,  5.12it/s]\u001b[A\n",
            " 61% 17/28 [00:03<00:02,  5.12it/s]\u001b[A\n",
            " 64% 18/28 [00:03<00:01,  5.12it/s]\u001b[A\n",
            " 68% 19/28 [00:03<00:01,  5.14it/s]\u001b[A\n",
            " 71% 20/28 [00:03<00:01,  5.14it/s]\u001b[A\n",
            " 75% 21/28 [00:04<00:01,  5.14it/s]\u001b[A\n",
            " 79% 22/28 [00:04<00:01,  5.13it/s]\u001b[A\n",
            " 82% 23/28 [00:04<00:00,  5.11it/s]\u001b[A\n",
            " 86% 24/28 [00:04<00:00,  5.12it/s]\u001b[A\n",
            " 89% 25/28 [00:04<00:00,  5.13it/s]\u001b[A\n",
            " 93% 26/28 [00:05<00:00,  5.12it/s]\u001b[A\n",
            " 96% 27/28 [00:05<00:00,  5.12it/s]\u001b[A\n",
            "100% 28/28 [00:05<00:00,  5.09it/s]\n",
            "epoch 2/10\n",
            "steps:  20% 164/820 [03:54<15:39,  1.43s/it, loss=0.15]saving checkpoint: /content/LoRA/output/zly_pro/zly_pro-000002.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 164\n",
            "prompt: masterpiece, best quality, 1girl, long hair, big eyes, looking at viewer, simple background  \n",
            "negative_prompt: EasyNegativeV2,(badhandv4:1.2),  \n",
            "height: 512\n",
            "width: 512\n",
            "sample_steps: 28\n",
            "scale: 7.0\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:05,  5.24it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:05,  5.16it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:04,  5.12it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:04,  5.12it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:04,  5.04it/s]\u001b[A\n",
            " 21% 6/28 [00:01<00:04,  5.06it/s]\u001b[A\n",
            " 25% 7/28 [00:01<00:04,  5.08it/s]\u001b[A\n",
            " 29% 8/28 [00:01<00:03,  5.08it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:03,  5.08it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:03,  5.08it/s]\u001b[A\n",
            " 39% 11/28 [00:02<00:03,  5.07it/s]\u001b[A\n",
            " 43% 12/28 [00:02<00:03,  5.09it/s]\u001b[A\n",
            " 46% 13/28 [00:02<00:02,  5.09it/s]\u001b[A\n",
            " 50% 14/28 [00:02<00:02,  5.09it/s]\u001b[A\n",
            " 54% 15/28 [00:02<00:02,  5.09it/s]\u001b[A\n",
            " 57% 16/28 [00:03<00:02,  5.07it/s]\u001b[A\n",
            " 61% 17/28 [00:03<00:02,  5.07it/s]\u001b[A\n",
            " 64% 18/28 [00:03<00:01,  5.08it/s]\u001b[A\n",
            " 68% 19/28 [00:03<00:01,  5.09it/s]\u001b[A\n",
            " 71% 20/28 [00:03<00:01,  5.08it/s]\u001b[A\n",
            " 75% 21/28 [00:04<00:01,  5.08it/s]\u001b[A\n",
            " 79% 22/28 [00:04<00:01,  5.07it/s]\u001b[A\n",
            " 82% 23/28 [00:04<00:00,  5.08it/s]\u001b[A\n",
            " 86% 24/28 [00:04<00:00,  5.08it/s]\u001b[A\n",
            " 89% 25/28 [00:04<00:00,  5.09it/s]\u001b[A\n",
            " 93% 26/28 [00:05<00:00,  5.08it/s]\u001b[A\n",
            " 96% 27/28 [00:05<00:00,  5.07it/s]\u001b[A\n",
            "100% 28/28 [00:05<00:00,  5.08it/s]\n",
            "epoch 3/10\n",
            "steps:  30% 246/820 [05:54<13:47,  1.44s/it, loss=0.148]saving checkpoint: /content/LoRA/output/zly_pro/zly_pro-000003.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 246\n",
            "prompt: masterpiece, best quality, 1girl, long hair, big eyes, looking at viewer, simple background  \n",
            "negative_prompt: EasyNegativeV2,(badhandv4:1.2),  \n",
            "height: 512\n",
            "width: 512\n",
            "sample_steps: 28\n",
            "scale: 7.0\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:05,  5.21it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:05,  5.17it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:04,  5.14it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:04,  5.12it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:04,  5.02it/s]\u001b[A\n",
            " 21% 6/28 [00:01<00:04,  5.05it/s]\u001b[A\n",
            " 25% 7/28 [00:01<00:04,  5.06it/s]\u001b[A\n",
            " 29% 8/28 [00:01<00:03,  5.07it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:03,  5.07it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:03,  5.06it/s]\u001b[A\n",
            " 39% 11/28 [00:02<00:03,  5.07it/s]\u001b[A\n",
            " 43% 12/28 [00:02<00:03,  5.08it/s]\u001b[A\n",
            " 46% 13/28 [00:02<00:02,  5.08it/s]\u001b[A\n",
            " 50% 14/28 [00:02<00:02,  5.09it/s]\u001b[A\n",
            " 54% 15/28 [00:02<00:02,  5.10it/s]\u001b[A\n",
            " 57% 16/28 [00:03<00:02,  5.07it/s]\u001b[A\n",
            " 61% 17/28 [00:03<00:02,  5.07it/s]\u001b[A\n",
            " 64% 18/28 [00:03<00:01,  5.07it/s]\u001b[A\n",
            " 68% 19/28 [00:03<00:01,  5.09it/s]\u001b[A\n",
            " 71% 20/28 [00:03<00:01,  5.09it/s]\u001b[A\n",
            " 75% 21/28 [00:04<00:01,  5.09it/s]\u001b[A\n",
            " 79% 22/28 [00:04<00:01,  5.07it/s]\u001b[A\n",
            " 82% 23/28 [00:04<00:00,  5.08it/s]\u001b[A\n",
            " 86% 24/28 [00:04<00:00,  5.07it/s]\u001b[A\n",
            " 89% 25/28 [00:04<00:00,  5.08it/s]\u001b[A\n",
            " 93% 26/28 [00:05<00:00,  5.07it/s]\u001b[A\n",
            " 96% 27/28 [00:05<00:00,  5.06it/s]\u001b[A\n",
            "100% 28/28 [00:05<00:00,  5.08it/s]\n",
            "epoch 4/10\n",
            "steps:  40% 328/820 [07:53<11:50,  1.44s/it, loss=0.144]saving checkpoint: /content/LoRA/output/zly_pro/zly_pro-000004.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 328\n",
            "prompt: masterpiece, best quality, 1girl, long hair, big eyes, looking at viewer, simple background  \n",
            "negative_prompt: EasyNegativeV2,(badhandv4:1.2),  \n",
            "height: 512\n",
            "width: 512\n",
            "sample_steps: 28\n",
            "scale: 7.0\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:05,  5.21it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:05,  5.17it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:04,  5.14it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:04,  5.11it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:04,  5.05it/s]\u001b[A\n",
            " 21% 6/28 [00:01<00:04,  5.07it/s]\u001b[A\n",
            " 25% 7/28 [00:01<00:04,  5.08it/s]\u001b[A\n",
            " 29% 8/28 [00:01<00:03,  5.08it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:03,  5.09it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:03,  5.07it/s]\u001b[A\n",
            " 39% 11/28 [00:02<00:03,  5.07it/s]\u001b[A\n",
            " 43% 12/28 [00:02<00:03,  5.07it/s]\u001b[A\n",
            " 46% 13/28 [00:02<00:02,  5.08it/s]\u001b[A\n",
            " 50% 14/28 [00:02<00:02,  5.08it/s]\u001b[A\n",
            " 54% 15/28 [00:02<00:02,  5.07it/s]\u001b[A\n",
            " 57% 16/28 [00:03<00:02,  5.06it/s]\u001b[A\n",
            " 61% 17/28 [00:03<00:02,  5.06it/s]\u001b[A\n",
            " 64% 18/28 [00:03<00:01,  5.06it/s]\u001b[A\n",
            " 68% 19/28 [00:03<00:01,  5.07it/s]\u001b[A\n",
            " 71% 20/28 [00:03<00:01,  5.06it/s]\u001b[A\n",
            " 75% 21/28 [00:04<00:01,  5.07it/s]\u001b[A\n",
            " 79% 22/28 [00:04<00:01,  5.08it/s]\u001b[A\n",
            " 82% 23/28 [00:04<00:00,  5.08it/s]\u001b[A\n",
            " 86% 24/28 [00:04<00:00,  5.09it/s]\u001b[A\n",
            " 89% 25/28 [00:04<00:00,  5.06it/s]\u001b[A\n",
            " 93% 26/28 [00:05<00:00,  5.07it/s]\u001b[A\n",
            " 96% 27/28 [00:05<00:00,  5.07it/s]\u001b[A\n",
            "100% 28/28 [00:05<00:00,  5.08it/s]\n",
            "epoch 5/10\n",
            "steps:  50% 410/820 [09:53<09:53,  1.45s/it, loss=0.154]saving checkpoint: /content/LoRA/output/zly_pro/zly_pro-000005.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 410\n",
            "prompt: masterpiece, best quality, 1girl, long hair, big eyes, looking at viewer, simple background  \n",
            "negative_prompt: EasyNegativeV2,(badhandv4:1.2),  \n",
            "height: 512\n",
            "width: 512\n",
            "sample_steps: 28\n",
            "scale: 7.0\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:05,  5.20it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:05,  5.12it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:04,  5.12it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:04,  5.10it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:04,  5.03it/s]\u001b[A\n",
            " 21% 6/28 [00:01<00:04,  5.05it/s]\u001b[A\n",
            " 25% 7/28 [00:01<00:04,  5.05it/s]\u001b[A\n",
            " 29% 8/28 [00:01<00:03,  5.07it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:03,  5.07it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:03,  5.07it/s]\u001b[A\n",
            " 39% 11/28 [00:02<00:03,  5.06it/s]\u001b[A\n",
            " 43% 12/28 [00:02<00:03,  5.07it/s]\u001b[A\n",
            " 46% 13/28 [00:02<00:02,  5.07it/s]\u001b[A\n",
            " 50% 14/28 [00:02<00:02,  5.08it/s]\u001b[A\n",
            " 54% 15/28 [00:02<00:02,  5.06it/s]\u001b[A\n",
            " 57% 16/28 [00:03<00:02,  5.06it/s]\u001b[A\n",
            " 61% 17/28 [00:03<00:02,  5.07it/s]\u001b[A\n",
            " 64% 18/28 [00:03<00:01,  5.08it/s]\u001b[A\n",
            " 68% 19/28 [00:03<00:01,  5.08it/s]\u001b[A\n",
            " 71% 20/28 [00:03<00:01,  5.08it/s]\u001b[A\n",
            " 75% 21/28 [00:04<00:01,  5.08it/s]\u001b[A\n",
            " 79% 22/28 [00:04<00:01,  5.08it/s]\u001b[A\n",
            " 82% 23/28 [00:04<00:00,  5.10it/s]\u001b[A\n",
            " 86% 24/28 [00:04<00:00,  5.09it/s]\u001b[A\n",
            " 89% 25/28 [00:04<00:00,  5.10it/s]\u001b[A\n",
            " 93% 26/28 [00:05<00:00,  5.09it/s]\u001b[A\n",
            " 96% 27/28 [00:05<00:00,  5.08it/s]\u001b[A\n",
            "100% 28/28 [00:05<00:00,  5.08it/s]\n",
            "epoch 6/10\n",
            "steps:  60% 492/820 [11:53<07:55,  1.45s/it, loss=0.148]saving checkpoint: /content/LoRA/output/zly_pro/zly_pro-000006.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 492\n",
            "prompt: masterpiece, best quality, 1girl, long hair, big eyes, looking at viewer, simple background  \n",
            "negative_prompt: EasyNegativeV2,(badhandv4:1.2),  \n",
            "height: 512\n",
            "width: 512\n",
            "sample_steps: 28\n",
            "scale: 7.0\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:05,  5.24it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:05,  5.16it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:04,  5.14it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:04,  5.14it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:04,  5.03it/s]\u001b[A\n",
            " 21% 6/28 [00:01<00:04,  5.06it/s]\u001b[A\n",
            " 25% 7/28 [00:01<00:04,  5.08it/s]\u001b[A\n",
            " 29% 8/28 [00:01<00:03,  5.09it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:03,  5.09it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:03,  5.06it/s]\u001b[A\n",
            " 39% 11/28 [00:02<00:03,  5.07it/s]\u001b[A\n",
            " 43% 12/28 [00:02<00:03,  5.07it/s]\u001b[A\n",
            " 46% 13/28 [00:02<00:02,  5.08it/s]\u001b[A\n",
            " 50% 14/28 [00:02<00:02,  5.07it/s]\u001b[A\n",
            " 54% 15/28 [00:02<00:02,  5.06it/s]\u001b[A\n",
            " 57% 16/28 [00:03<00:02,  5.06it/s]\u001b[A\n",
            " 61% 17/28 [00:03<00:02,  5.08it/s]\u001b[A\n",
            " 64% 18/28 [00:03<00:01,  5.07it/s]\u001b[A\n",
            " 68% 19/28 [00:03<00:01,  5.08it/s]\u001b[A\n",
            " 71% 20/28 [00:03<00:01,  5.06it/s]\u001b[A\n",
            " 75% 21/28 [00:04<00:01,  5.07it/s]\u001b[A\n",
            " 79% 22/28 [00:04<00:01,  5.07it/s]\u001b[A\n",
            " 82% 23/28 [00:04<00:00,  5.08it/s]\u001b[A\n",
            " 86% 24/28 [00:04<00:00,  5.08it/s]\u001b[A\n",
            " 89% 25/28 [00:04<00:00,  5.08it/s]\u001b[A\n",
            " 93% 26/28 [00:05<00:00,  5.08it/s]\u001b[A\n",
            " 96% 27/28 [00:05<00:00,  5.07it/s]\u001b[A\n",
            "100% 28/28 [00:05<00:00,  5.08it/s]\n",
            "epoch 7/10\n",
            "steps:  70% 574/820 [13:52<05:56,  1.45s/it, loss=0.147]saving checkpoint: /content/LoRA/output/zly_pro/zly_pro-000007.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 574\n",
            "prompt: masterpiece, best quality, 1girl, long hair, big eyes, looking at viewer, simple background  \n",
            "negative_prompt: EasyNegativeV2,(badhandv4:1.2),  \n",
            "height: 512\n",
            "width: 512\n",
            "sample_steps: 28\n",
            "scale: 7.0\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:05,  5.26it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:05,  5.15it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:04,  5.13it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:04,  5.11it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:04,  5.02it/s]\u001b[A\n",
            " 21% 6/28 [00:01<00:04,  5.04it/s]\u001b[A\n",
            " 25% 7/28 [00:01<00:04,  5.05it/s]\u001b[A\n",
            " 29% 8/28 [00:01<00:03,  5.07it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:03,  5.08it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:03,  5.08it/s]\u001b[A\n",
            " 39% 11/28 [00:02<00:03,  5.08it/s]\u001b[A\n",
            " 43% 12/28 [00:02<00:03,  5.06it/s]\u001b[A\n",
            " 46% 13/28 [00:02<00:02,  5.06it/s]\u001b[A\n",
            " 50% 14/28 [00:02<00:02,  5.08it/s]\u001b[A\n",
            " 54% 15/28 [00:02<00:02,  5.08it/s]\u001b[A\n",
            " 57% 16/28 [00:03<00:02,  5.07it/s]\u001b[A\n",
            " 61% 17/28 [00:03<00:02,  5.08it/s]\u001b[A\n",
            " 64% 18/28 [00:03<00:01,  5.08it/s]\u001b[A\n",
            " 68% 19/28 [00:03<00:01,  5.09it/s]\u001b[A\n",
            " 71% 20/28 [00:03<00:01,  5.08it/s]\u001b[A\n",
            " 75% 21/28 [00:04<00:01,  5.08it/s]\u001b[A\n",
            " 79% 22/28 [00:04<00:01,  5.07it/s]\u001b[A\n",
            " 82% 23/28 [00:04<00:00,  5.07it/s]\u001b[A\n",
            " 86% 24/28 [00:04<00:00,  5.07it/s]\u001b[A\n",
            " 89% 25/28 [00:04<00:00,  5.08it/s]\u001b[A\n",
            " 93% 26/28 [00:05<00:00,  5.08it/s]\u001b[A\n",
            " 96% 27/28 [00:05<00:00,  5.08it/s]\u001b[A\n",
            "100% 28/28 [00:05<00:00,  5.08it/s]\n",
            "epoch 8/10\n",
            "steps:  80% 656/820 [15:52<03:58,  1.45s/it, loss=0.155]saving checkpoint: /content/LoRA/output/zly_pro/zly_pro-000008.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 656\n",
            "prompt: masterpiece, best quality, 1girl, long hair, big eyes, looking at viewer, simple background  \n",
            "negative_prompt: EasyNegativeV2,(badhandv4:1.2),  \n",
            "height: 512\n",
            "width: 512\n",
            "sample_steps: 28\n",
            "scale: 7.0\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:05,  5.13it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:05,  5.11it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:04,  5.08it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:04,  5.08it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:04,  5.01it/s]\u001b[A\n",
            " 21% 6/28 [00:01<00:04,  5.03it/s]\u001b[A\n",
            " 25% 7/28 [00:01<00:04,  5.06it/s]\u001b[A\n",
            " 29% 8/28 [00:01<00:03,  5.06it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:03,  5.07it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:03,  5.06it/s]\u001b[A\n",
            " 39% 11/28 [00:02<00:03,  5.06it/s]\u001b[A\n",
            " 43% 12/28 [00:02<00:03,  5.08it/s]\u001b[A\n",
            " 46% 13/28 [00:02<00:02,  5.08it/s]\u001b[A\n",
            " 50% 14/28 [00:02<00:02,  5.08it/s]\u001b[A\n",
            " 54% 15/28 [00:02<00:02,  5.07it/s]\u001b[A\n",
            " 57% 16/28 [00:03<00:02,  5.08it/s]\u001b[A\n",
            " 61% 17/28 [00:03<00:02,  5.10it/s]\u001b[A\n",
            " 64% 18/28 [00:03<00:01,  5.09it/s]\u001b[A\n",
            " 68% 19/28 [00:03<00:01,  5.09it/s]\u001b[A\n",
            " 71% 20/28 [00:03<00:01,  5.06it/s]\u001b[A\n",
            " 75% 21/28 [00:04<00:01,  5.06it/s]\u001b[A\n",
            " 79% 22/28 [00:04<00:01,  5.06it/s]\u001b[A\n",
            " 82% 23/28 [00:04<00:00,  5.05it/s]\u001b[A\n",
            " 86% 24/28 [00:04<00:00,  5.04it/s]\u001b[A\n",
            " 89% 25/28 [00:04<00:00,  5.05it/s]\u001b[A\n",
            " 93% 26/28 [00:05<00:00,  5.05it/s]\u001b[A\n",
            " 96% 27/28 [00:05<00:00,  5.06it/s]\u001b[A\n",
            "100% 28/28 [00:05<00:00,  5.07it/s]\n",
            "epoch 9/10\n",
            "steps:  90% 738/820 [17:51<01:59,  1.45s/it, loss=0.14] saving checkpoint: /content/LoRA/output/zly_pro/zly_pro-000009.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 738\n",
            "prompt: masterpiece, best quality, 1girl, long hair, big eyes, looking at viewer, simple background  \n",
            "negative_prompt: EasyNegativeV2,(badhandv4:1.2),  \n",
            "height: 512\n",
            "width: 512\n",
            "sample_steps: 28\n",
            "scale: 7.0\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:05,  5.23it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:05,  5.15it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:04,  5.12it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:04,  5.12it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:04,  5.05it/s]\u001b[A\n",
            " 21% 6/28 [00:01<00:04,  5.07it/s]\u001b[A\n",
            " 25% 7/28 [00:01<00:04,  5.10it/s]\u001b[A\n",
            " 29% 8/28 [00:01<00:03,  5.10it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:03,  5.11it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:03,  5.08it/s]\u001b[A\n",
            " 39% 11/28 [00:02<00:03,  5.08it/s]\u001b[A\n",
            " 43% 12/28 [00:02<00:03,  5.07it/s]\u001b[A\n",
            " 46% 13/28 [00:02<00:02,  5.09it/s]\u001b[A\n",
            " 50% 14/28 [00:02<00:02,  5.09it/s]\u001b[A\n",
            " 54% 15/28 [00:02<00:02,  5.10it/s]\u001b[A\n",
            " 57% 16/28 [00:03<00:02,  5.09it/s]\u001b[A\n",
            " 61% 17/28 [00:03<00:02,  5.07it/s]\u001b[A\n",
            " 64% 18/28 [00:03<00:01,  5.08it/s]\u001b[A\n",
            " 68% 19/28 [00:03<00:01,  5.09it/s]\u001b[A\n",
            " 71% 20/28 [00:03<00:01,  5.09it/s]\u001b[A\n",
            " 75% 21/28 [00:04<00:01,  5.07it/s]\u001b[A\n",
            " 79% 22/28 [00:04<00:01,  5.07it/s]\u001b[A\n",
            " 82% 23/28 [00:04<00:00,  5.09it/s]\u001b[A\n",
            " 86% 24/28 [00:04<00:00,  5.08it/s]\u001b[A\n",
            " 89% 25/28 [00:04<00:00,  5.10it/s]\u001b[A\n",
            " 93% 26/28 [00:05<00:00,  5.09it/s]\u001b[A\n",
            " 96% 27/28 [00:05<00:00,  5.07it/s]\u001b[A\n",
            "100% 28/28 [00:05<00:00,  5.09it/s]\n",
            "epoch 10/10\n",
            "steps: 100% 820/820 [19:50<00:00,  1.45s/it, loss=0.129]generating sample images at step / サンプル画像生成 ステップ: 820\n",
            "prompt: masterpiece, best quality, 1girl, long hair, big eyes, looking at viewer, simple background  \n",
            "negative_prompt: EasyNegativeV2,(badhandv4:1.2),  \n",
            "height: 512\n",
            "width: 512\n",
            "sample_steps: 28\n",
            "scale: 7.0\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:05,  5.22it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:05,  5.15it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:04,  5.12it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:04,  5.10it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:04,  5.07it/s]\u001b[A\n",
            " 21% 6/28 [00:01<00:04,  5.09it/s]\u001b[A\n",
            " 25% 7/28 [00:01<00:04,  5.08it/s]\u001b[A\n",
            " 29% 8/28 [00:01<00:03,  5.08it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:03,  5.07it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:03,  5.08it/s]\u001b[A\n",
            " 39% 11/28 [00:02<00:03,  5.07it/s]\u001b[A\n",
            " 43% 12/28 [00:02<00:03,  5.08it/s]\u001b[A\n",
            " 46% 13/28 [00:02<00:02,  5.09it/s]\u001b[A\n",
            " 50% 14/28 [00:02<00:02,  5.08it/s]\u001b[A\n",
            " 54% 15/28 [00:02<00:02,  5.09it/s]\u001b[A\n",
            " 57% 16/28 [00:03<00:02,  5.08it/s]\u001b[A\n",
            " 61% 17/28 [00:03<00:02,  5.08it/s]\u001b[A\n",
            " 64% 18/28 [00:03<00:01,  5.09it/s]\u001b[A\n",
            " 68% 19/28 [00:03<00:01,  5.09it/s]\u001b[A\n",
            " 71% 20/28 [00:03<00:01,  5.10it/s]\u001b[A\n",
            " 75% 21/28 [00:04<00:01,  5.09it/s]\u001b[A\n",
            " 79% 22/28 [00:04<00:01,  5.10it/s]\u001b[A\n",
            " 82% 23/28 [00:04<00:00,  5.08it/s]\u001b[A\n",
            " 86% 24/28 [00:04<00:00,  5.07it/s]\u001b[A\n",
            " 89% 25/28 [00:04<00:00,  5.08it/s]\u001b[A\n",
            " 93% 26/28 [00:05<00:00,  5.08it/s]\u001b[A\n",
            " 96% 27/28 [00:05<00:00,  5.08it/s]\u001b[A\n",
            "100% 28/28 [00:05<00:00,  5.08it/s]\n",
            "save trained model to /content/LoRA/output/zly_pro/zly_pro.safetensors\n",
            "model saved.\n",
            "steps: 100% 820/820 [19:57<00:00,  1.46s/it, loss=0.129]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LoRA/output/zly_pro/目录下的safetensor文件就是我们得到的LoRA模型，下载到WebUI就可以直接使用。"
      ],
      "metadata": {
        "id": "-lW9BqxjCHjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "模型推理"
      ],
      "metadata": {
        "id": "C9v0773zVlDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "network_weight = \"/content/LoRA/output/zly_pro/zly_pro.safetensors\"\n",
        "# network_weight = \"/content/LoRA/output/zly_pro/zly_pro-000009.safetensors\"\n",
        "network_mul = 1\n",
        "network_module = \"networks.lora\"\n",
        "network_args = \"\"\n",
        "\n",
        "v2 = False\n",
        "v_parameterization = False\n",
        "prompt = \"masterpiece, best quality, 1girl, big eyes, white cape with a white furry collar, looking at viewer, simple background\"   # 测试prompt\n",
        "negative = \"lowres, bad anatomy, ugly face, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\"\n",
        "model = pretrained_model_name_or_path\n",
        "vae = \"\"\n",
        "outdir = \"/content/tmp\"  # 图片存储的路径\n",
        "# scale = 7\n",
        "scale = 8\n",
        "sampler = \"euler_a\"\n",
        "# sampler = \"ddim\"\n",
        "steps = 20\n",
        "# steps = 25\n",
        "precision = \"fp16\"\n",
        "width = 512\n",
        "height = 768\n",
        "images_per_prompt = 6\n",
        "batch_size = 8\n",
        "clip_skip = 2\n",
        "seed = 3412\n",
        "\n",
        "final_prompt = f\"{prompt} --n {negative}\"\n",
        "\n",
        "config = {\n",
        "    \"v2\": v2,\n",
        "    \"v_parameterization\": v_parameterization,\n",
        "    \"network_module\": network_module,\n",
        "    \"network_weight\": network_weight,\n",
        "    \"network_mul\": float(network_mul),\n",
        "    \"network_args\": eval(network_args) if network_args else None,\n",
        "    \"ckpt\": model,\n",
        "    \"outdir\": outdir,\n",
        "    \"xformers\": True,\n",
        "    \"vae\": vae if vae else None,\n",
        "    \"fp16\": True,\n",
        "    \"W\": width,\n",
        "    \"H\": height,\n",
        "    \"seed\": seed if seed > 0 else None,\n",
        "    \"scale\": scale,\n",
        "    \"sampler\": sampler,\n",
        "    \"steps\": steps,\n",
        "    \"max_embeddings_multiples\": 3,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"images_per_prompt\": images_per_prompt,\n",
        "    \"clip_skip\": clip_skip if not v2 else None,\n",
        "    \"prompt\": final_prompt,\n",
        "}\n",
        "\n",
        "args = \"\"\n",
        "for k, v in config.items():\n",
        "    if k.startswith(\"_\"):\n",
        "        args += f'\"{v}\" '\n",
        "    elif isinstance(v, str):\n",
        "        args += f'--{k}=\"{v}\" '\n",
        "    elif isinstance(v, bool) and v:\n",
        "        args += f\"--{k} \"\n",
        "    elif isinstance(v, float) and not isinstance(v, bool):\n",
        "        args += f\"--{k}={v} \"\n",
        "    elif isinstance(v, int) and not isinstance(v, bool):\n",
        "        args += f\"--{k}={v} \"\n",
        "\n",
        "final_args = f\"python gen_img_diffusers.py {args}\"\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "!{final_args}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMetKpLVNr79",
        "outputId": "b0522724-7fbe-4be3-dc25-012b13d5eb8a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.16) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
            "load StableDiffusion checkpoint\n",
            "loading u-net: <All keys matched successfully>\n",
            "loading vae: <All keys matched successfully>\n",
            "loading text encoder: <All keys matched successfully>\n",
            "Replace CrossAttention.forward to use NAI style Hypernetwork and xformers\n",
            "loading tokenizer\n",
            "prepare tokenizer\n",
            "import network module: networks.lora\n",
            "load network weights from: /content/LoRA/output/zly_pro/zly_pro.safetensors\n",
            "create LoRA network from weights\n",
            "create LoRA for Text Encoder: 72 modules.\n",
            "create LoRA for U-Net: 192 modules.\n",
            "enable LoRA for text encoder\n",
            "enable LoRA for U-Net\n",
            "weights are loaded: <All keys matched successfully>\n",
            "pipeline is ready.\n",
            "iteration 1/1\n",
            "prompt 1/1: masterpiece, best quality, 1girl, big eyes, white cape with a white furry collar, looking at viewer, simple background\n",
            "negative prompt: lowres, bad anatomy, ugly face, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\n",
            "100% 20/20 [00:32<00:00,  1.61s/it]\n",
            "done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "/content/tmp查看生成的图片！更换代码里的prompt反复尝试几次。权重和图片要保存本地，不然colab断开就都没了。"
      ],
      "metadata": {
        "id": "Ol9_GsBhDRHI"
      }
    }
  ]
}